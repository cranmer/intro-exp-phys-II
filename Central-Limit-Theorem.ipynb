{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Central Limit Theorem\n",
    "\n",
    "The [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) is a key result in probability theory that helps explain why normal, or Gaussian, distributions are so omnipresent. The setup is that you have distributions for $N$ random variables $x_i$ and you want to know the distribution of $q = \\sum_{i=1}^{N} x_i$. Think of each $x_i$ as coming from it's own distribution like in the figure below. For instance, $x_1$ might be the weight of spoons, $x_2$ the weight of forks, $x_3$ the weight of bowls, ..., and $x_N$ of plates in your kitchen. Then $q$ would represent the total weight when you have one of each of those objects. The distribution of weights for each object might be weird because you have some mix-and-match set of silverware from your parents, grandparents, IKEA, and the thrift shop. The *central limit theorem* says that if you have enough objects (i.e. $N$ is large), then $q$ has a normal (Gaussian) distribution.\n",
    "\n",
    "![](Central_limit_theorem.png)\n",
    "\n",
    "Moreover, the central limit theorem states that the mean value of $q$ is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\mu_{q} = \\sum_{i=1}^{N} \\mu_{x_i} \n",
    "\\end{equation}\n",
    "\n",
    "and the variance (standard deviation squared) is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_{q}^{2} = \\sum_{i=1}^{N} \\sigma^2_{x_i} \n",
    "\\end{equation}\n",
    "\n",
    "*if you are having problems with the math displaing, click [here](http://nbviewer.jupyter.org/github/cranmer/intro-exp-phys-II/blob/master/Central-Limit-Theorem.ipynb?flush_cache=true)*\n",
    "\n",
    "The mean probably isn't surprising because $q$ is just a sum and the integral the defines the mean just distributes across each term. Also, the equation for the variance is the same as the propagation of errors formula we use when we add different measurements together. However, that propoagation of errors formula is derived from the Central Limit Theorem, not vice versa.\n",
    "\n",
    "### This is a collaborative project\n",
    "\n",
    "In this repository there is a folder called `distributions` with several python files. The idea is that each student will create one of these python files and we will use GitHub to collect them. Each of these files has a python class that describes a probability distribution.  Each of these classes will define:\n",
    "   * `x_min, x_max, f_max` - used for the accept/reject Monte Carlo sampling algorithm\n",
    "   * `pdf()` - the probability density function\n",
    "   * `mean()` - the mean of the pdf\n",
    "   * `std()` - the standard deviation of the pdf\n",
    "   \n",
    "In addition, each of these python classes inherits from `BaseDistribution` which knows how to run the accept/reject algorithm using the information above ([see inside](distributions/base_distribution.py) ). In order to generate `n_samples` from the pdf, you simply call `dist.rvs(n_samples)`, where `dist` is an instance of one of these python classes.\n",
    "\n",
    "**Naming Convention**: Name your file `Dist_<netid>.py` and your distribution the same way (without the `.py`). If you want to contribute more than one distribution, you can can add `Dist_<netid>_<index>.py`, where `<index>` is 1,2,3,...\n",
    "\n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat distributions/Dist_kc90.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage of the distributions\n",
    "\n",
    "Ok, now let's use them. So far there are only two distributions, but there will be more soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline --no-import-all\n",
    "from scipy.stats import norm #will use this for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import all our distributions\n",
    "import distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some funny python to make a list of all the distributions for convenience\n",
    "all_distributions_dict = dict([(name, cls) for name, cls in distributions.__dict__.items() if isinstance(cls, type)])\n",
    "all_distributions_list = [(cls) for name, cls in distributions.__dict__.items() if isinstance(cls, type)]\n",
    "# print out their names\n",
    "all_distributions_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(all_distributions_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Do tests\n",
    "ok_distributions_list=[]\n",
    "problems=[]\n",
    "for i, cls in enumerate(all_distributions_list):\n",
    "    #print(cls)\n",
    "    try:\n",
    "        dist = cls()\n",
    "        N_test = 100000\n",
    "        #print('will try to generate for %s' %(cls.__name__))\n",
    "        if dist.pdf(dist.x_min + .3*(dist.x_max-dist.x_min)) < 1E-3:\n",
    "            print(\"may have a problem\")\n",
    "            continue\n",
    "            \n",
    "        rvs = dist.rvs(N_test)\n",
    "        if np.abs(np.mean(rvs) - dist.mean()) > 5*np.std(rvs)/np.sqrt(N_test):\n",
    "            print(\"means don't match for %s: %f vs. %f\" %(cls.__name__, \n",
    "                                                          np.mean(rvs), dist.mean()))\n",
    "            continue\n",
    "            \n",
    "        elif np.abs(np.std(rvs) - dist.std()) > 5*np.std(rvs)/np.sqrt(np.sqrt(1.*N_test)):\n",
    "            print(\"std devs. don't match for %s: %f vs. %f\" %(cls.__name__, \n",
    "                                                          np.std(rvs), dist.std()))\n",
    "            continue\n",
    "        elif np.abs(np.std(rvs) - dist.std()) / dist.std() > 0.1:\n",
    "            print(\"std devs. don't match for %s: %f vs. %f\" %(cls.__name__, \n",
    "                                                          np.std(rvs), dist.std()))\n",
    "            continue\n",
    "        \n",
    "        elif np.sum(dist.pdf(np.linspace(dist.x_min,dist.x_max,100))<0) > 0:\n",
    "            print(\"pdf was negative in some places\")\n",
    "            continue                    \n",
    "\n",
    "        else:\n",
    "            print(\"%s passes tests, adding it\" %(cls.__name__))\n",
    "            ok_distributions_list.append(cls)\n",
    "    except:\n",
    "        print(\"%s has errors, does't work\" %(cls.__name__))\n",
    "        continue\n",
    "\n",
    "print(\"list of ok distributions:\",[i.__name__ for i in ok_distributions_list])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "problems = [x for x in all_distributions_list if x not in ok_distributions_list]\n",
    "[i.__name__ for i in problems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how many samples for plots?\n",
    "n_samples = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here's an example of usage\n",
    "dist = distributions.Dist_ac5790()\n",
    "rvs = dist.rvs(n_samples)\n",
    "counts, bins, edges = plt.hist(rvs, bins=50, normed=True, alpha =0.2)\n",
    "y = []\n",
    "for bin in bins:\n",
    "    y.append(dist.pdf(bin))\n",
    "plt.plot(bins, y, c='r', lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's inspect all the distributions we have\n",
    "\n",
    "Here we will loop over the different distributions and make a plot like the one above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=2*plt.figaspect(len(ok_distributions_list)))\n",
    "for i, cls in enumerate(ok_distributions_list):\n",
    "    dist = cls()\n",
    "    rvs = dist.rvs(10000)\n",
    "    ax = fig.add_subplot(len(ok_distributions_list),1,i+1)\n",
    "    counts, bins, patches = ax.hist(rvs, bins=50, normed=True, alpha=0.2)\n",
    "    # depending on how you code up your pdf() function, numpy might not\n",
    "    # be able to \"broadcast\" (apply the function for each element of a numpy array)\n",
    "    # but this should always wrok\n",
    "    y = []\n",
    "    for bin in bins:\n",
    "        y.append(dist.pdf(bin))\n",
    "        \n",
    "    # ok, now plot it\n",
    "    plt.plot(bins, y, c='r', lw=2)\n",
    "    \n",
    "    # and let's check if the distribution is ok\n",
    "    print(\"%s: std from samples = %.2f, std from dist = %.2f\" %(cls.__name__,np.std(dist.rvs(n_samples)), dist.std()))\n",
    "    if np.abs(np.std(dist.rvs(n_samples)) - dist.std()) / dist.std() > 0.1:\n",
    "        print(\"looks like a problem with this distribution: \", cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Demonstration of the Central Limit Theorem\n",
    "\n",
    "ok, let's use one of the distributions to demonstrate the central limit theorem. We will use the same distribution $N$ times.\n",
    "\n",
    "First let's make a little helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_convolution(dist, N):\n",
    "    n_samples = 10000\n",
    "    q = np.zeros(n_samples)\n",
    "    var_q = 0.\n",
    "    mean_q = 0.\n",
    "\n",
    "    for i in range(N):\n",
    "        q = q+dist.rvs(n_samples)\n",
    "        var_q = var_q + dist.std()**2\n",
    "        mean_q = mean_q + dist.mean()\n",
    "\n",
    "    std_q = np.sqrt( var_q )\n",
    "\n",
    "\n",
    "    counts, bins, patches = plt.hist(q,bins=50, normed=True, alpha=.2)\n",
    "    plt.plot(bins, norm.pdf(bins,loc=mean_q, scale=std_q), lw=2, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use it for $N=\\{2,4,32\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist = distributions.Dist_nj18()\n",
    "do_convolution(dist,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_convolution(dist,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_convolution(dist,32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Gorgeous!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's do the same thing randomly using different distributions \n",
    "\n",
    "To do this we will use `np.random.choice` to randomly choose from our list. Here's an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.choice(['a','b','c','d'], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a variation on the helper function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_random_convolution(list_of_distributions, N):\n",
    "    n_samples = 10000\n",
    "    q = np.zeros(n_samples)\n",
    "    var_q = 0.\n",
    "    mean_q = 0.\n",
    "\n",
    "    for dist_class in np.random.choice(list_of_distributions,N):\n",
    "        dist = dist_class()\n",
    "        print(dist_class.__name__, dist.std())\n",
    "        q = q+dist.rvs(n_samples)\n",
    "        var_q = var_q + dist.std()**2\n",
    "        mean_q = mean_q + dist.mean()\n",
    "\n",
    "    std_q = np.sqrt( var_q )\n",
    "\n",
    "\n",
    "    counts, bins, patches = plt.hist(q,bins=50, normed=True, alpha=.2)\n",
    "    plt.plot(bins, norm.pdf(bins,loc=mean_q, scale=std_q), lw=2, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_random_convolution(ok_distributions_list,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_random_convolution(ok_distributions_list,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_random_convolution(ok_distributions_list,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "do_random_convolution(ok_distributions_list,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2 of project\n",
    "\n",
    "## a) preliminaries\n",
    "From either master or the branch you used to submit your distribution, update so that you have the current version of `cranmer/intro-exp-phys-II`.\n",
    "\n",
    "You can either do this in GitHub desktop by finding the button near the top left or by typing this:\n",
    "```\n",
    "git fetch cranmer master\n",
    "git merge cranmer/master\n",
    "```\n",
    "\n",
    "Now Create a new branch called \"part2\"\n",
    "\n",
    "\n",
    "## b) execute the notebook above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Make a $\\chi^2$ function\n",
    "\n",
    "Below is a copy of the `do_random_convolution` function with a new name. Modify it so taht it returns the chi-square quantity that says how closely the observed distribution matches the prediction from the Central Limit theorem.\n",
    "\n",
    "YOu might want to check out the [chi-square-of-distribution](chi-square-of-distribution.ipynb) notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_random_convolution_with_chi2(list_of_distributions, N):\n",
    "    n_samples = 100000\n",
    "    q = np.zeros(n_samples)\n",
    "    var_q = 0.\n",
    "    mean_q = 0.\n",
    "\n",
    "    for dist_class in np.random.choice(list_of_distributions,N):\n",
    "        dist = dist_class()\n",
    "        q = q+dist.rvs(n_samples)\n",
    "        var_q = var_q + dist.std()**2\n",
    "        mean_q = mean_q + dist.mean()\n",
    "\n",
    "    std_q = np.sqrt( var_q )\n",
    "\n",
    "\n",
    "    counts, bins, patches = plt.hist(q,bins=50, normed=True, alpha=.2)\n",
    "    plt.plot(bins, norm.pdf(bins,loc=mean_q, scale=std_q), lw=2, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Make a plot\n",
    "\n",
    "Plot the $\\chi^2$ quantity vs. N for N=2,4,8,16,32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) push a new version of this notebook to GitHub\n",
    "\n",
    "Execute the notebook, get your plots, save it, commit the changes to the `part2` branch, and then push (sync) to GitHub.com.  When you are done, make a new pull request.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
